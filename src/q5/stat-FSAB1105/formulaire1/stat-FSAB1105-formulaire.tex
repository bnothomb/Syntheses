\documentclass[10pt,landscape,en,license=none]{../../../eplformulaire}

\usepackage{../stat-FSAB1105}

\usepackage[left=0.1cm,right=0.1cm,top=0.1cm,bottom=0.2cm]{geometry}
\usepackage{empheq}

\setlength{\columnseprule}{1pt}
\def\columnseprulecolor{\color{black}}

\newcommand{\indepp}{\rotatebox[origin=c]{90}{$\models$}}
\newcommand{\indeppp}{\rotatebox[origin=c]{90}{$\vDash$}}
\DeclareMathOperator{\indep}{\perp \!\!\! \perp}

\hypertitle{Probabilité et Statistiques}{5}{FSAB}{1105}
{Charles Momin}
{Anour El Ghouch et Rainer von Sachs}

\begin{multicols*}{4}
\scriptsize

\textbf{\footnotesize Data set properties}

\begin{itemize}
\item{\textbf{Moyenne}: $\bar{X} = \dfrac{1}{n} \displaystyle{\sum^{n}_{i=\color{red}{1}} X_i }$}
\item{\textbf{Variance}: $\sigma^2 = \dfrac{1}{n-1} \displaystyle{\sum_{i=1}^n \left( X_i - \overline{X}\right) ^2}$ }
\item{\textbf{Écart type}: $\sigma = \sqrt{\sigma^2}$}
\item{\textbf{Médiane}: $q_{0.5}$}
\item{\textbf{Quantile}: \\ for a sample of $n$ data \\ $q_p = X \left( 1 + (n-1)\cdot p\right) = F^{-1}(p)$}
		\\IQR (inter quantile range) = $q_{0.75}-q_{0.25}$
\item{\textbf{outlier}: $ < q_{0.25} - 1.5 IQR$ || $> q_{0.75} + 1.5 IQR$}
\item{\textbf{coef variation}: $CV = \frac{s}{\overline{X}}$}
\end{itemize}

\textbf{\footnotesize Probability properties}

\begin{itemize}
\item[\textbullet]{\textbf{$P(A) = 1 - P \left( \overline{A} \right)$}}
\item[\textbullet]{\textbf{$P(A) = P \left( A \cap B \right)+ P \left( A \cap \overline{B} \right) $}}
\item[\textbullet]{\textbf{$P(A\mid B) = \dfrac{P(A \cap B)}{P(B)}$}}
\item[$\Rightarrow$]{$P(A \cap B) = P(B)P(A|B) = P(B)P(B|A)$}
\item[\textbullet]{\textbf{$P \left( A\cap B\right) = P(A) \cdot P(B)$ (définition indep.)}}
\item[$\Rightarrow$]{\textbf{$P(A\mid B) = P(A)$ (iff indépendant $A \indep B$)} }
\item[\textbullet]{\textbf{$P(B\mid A) = \dfrac{P(A\mid B) \cdot P(B)}{P(A)}$}}
\item[\textbullet]{\textbf{$P(A_i\mid B) = \dfrac{P(B \mid A_i)P(A_i)}{\sum P(B\mid A_i)P(A_i)}$}}
\item[\textbullet]{$P(A \cup B) = P(A) + P(B) - P(A \cap B)$}
\item[\textbullet]{A,B incompatible,disjoint,exclu mutuel:\\ $P(A \cap B)=\{\varnothing\}$}
\item[\textbullet]{$P(\overline{A \cup B}) = P(\overline{A} \cap \overline{B})$} (morgan)
\item[\textbullet]{$P(\overline{A \cap B}) = P(\overline{A} \cup \overline{B})$} (morgan)
\end{itemize}
 
\textbf{\footnotesize Enumerative combinations}
$r$ objects taken from $n$ distinct objects
\begin{itemize}
\item Order, replacement: $n^k$
\item{\textbf{Permutation:} order, no replacement, $k = n$: $n!$ }
\item{\textbf{Partial permutation/Arrangement:} \\ order, no replacement\\ $A_n^r = P_n^r = \dfrac{n!}{(n-r)!}$ \\ if replacement possible permutations = $n^r$}
\item{\textbf{Combination:} \\ 
	no order, no replacement \\ 
	$C_n^r = \dfrac{n!}{r!(n-r)!} = 
	\left( \begin{array}{cc}
	n \\
	r
	\end{array} \right)$
}
\end{itemize}

\columnbreak
\textbf{\footnotesize Random variable (r.v.) properties}

\begin{itemize}
\item iid = indépendant et identiquement distribué 
\item{\textbf{Probability mass fct PMF (discrete r.v.):} 
	\\$p(x) = P(X=x), 0 \leq p(x) \leq 1$}
\item{\textbf{Probability density fct PDF(continous r.v.):} 
	\\ f(x) ($\geq$ 0) as $P(X\in I) = \displaystyle{\int_I f(x) ~\textrm{d}x}$}
\item{\textbf{cumulative distribution fct CDF:} 
	\\ $F(a) = P(X \leq a) = \int_{-\infty}^a f(x) \dif x$}
\item{f(x) = F'(x)}
\item{ $P(X \in [a,b]) = P(a \leq X \leq b) 
	\\ = P(X \leq b) - P(X \leq a) = F(b) - F(a)
	\\ = \displaystyle{\int_a^b f(x) ~\textrm{d}x}$ en continu
	\\ $= \displaystyle{\sum_{i \in [a,b]} p(x_i)}$ en discret}
\item{\textbf{Espérance E(X)}:\\
	\textbullet $\displaystyle{\E(x) = \int_{-\infty}^\infty x f(x) dx}$ ou $\displaystyle{\sum_x x p(x)}$\\
    \textbullet $\displaystyle{\E(1) = \int_{-\infty}^\infty f(x) dx = 1}$ ou $\displaystyle{\sum_x p(x) = 1}$\\
    \textbullet $\E(g(X)) = \displaystyle{\int_{-\infty}^{\infty}} g(x) \cdot f(x) \dif x$ ou $\displaystyle{\sum_{x} g(x) \cdot  p(x)}$ \\
	\textbullet $\mu_X = \E(X)$ \\
	\textit{Property:} \\ 
	\textbullet $\E(aX + bY + d) = a \E(X) + b\E(Y) + d$ \\
	\textbullet $E \left(\sum_i X_i\right) = \sum_i \E(X_i)$ \\
	\textbullet $\E(XY) \neq \E(X)\E(Y)$ \\
	\textbullet $\E(h(X)) \neq h(E(X))$}
\item{\textbf{Variance V(X):} \\ 
	\textbullet $\sigma^2_X = \E((X-\mu_X)^2) = \E(X^2) - (\E(X))^2$ \\ 
	standard deviation$=\sqrt{\sigma^2_X}$ \\ 			
	\textit{Property} \\ 
	\textbullet $\var(a) =0$ \\
  \textbullet $\var(a+bX+cY)=b^2 \var(X) + c^2 \var(Y) + 2bc \cov(X,Y)$\\
  \textbullet $\var(X+Y) \neq \var(X)+\var(Y)$}
\item{\textbf{Fct génératrice de moment MGF:} \\ $m_X(t) = \E(\exp(tX))$ \\ 
	(k-om) $\displaystyle{\left.\dfrac{d^k m_X(t)}{dt^k}\right|_{t=0} = \E(X^k)}$\\
	$m_{a+bX}= \exp(at) m_X(bt)$}
\end{itemize}


\textbf{\footnotesize Discrete distributions}

\begin{itemize}
\item{\textbf{Uniform:} \\ \textbullet $P(X=x) = \dfrac{1}{n}$ \\ \textbullet $\mu_X = \dfrac{X_1 + ... + X_n}{n}$ \\ \textbullet $\sigma_X^2 = \dfrac{X_1^2 + ... + X_n^2}{n} - \mu_X^2$}
\item{\textbf{Bernouilli:} \\
	(UN essai avec réussite(1) ou échec(0)) \\ 
	$X\sim Be(p)$ \\ 
	\textbullet $P(X=x)=p^x(1-p)^{1-x}$ \\ 
	\textbullet $\mu_X = p$ \\ 
	\textbullet $\sigma_X^2 = pq = p(1-p)$}
\item{\textbf{Binomial (nbr succ. in $n$ trials):} \\ 
	$X\sim Bi(n,p)$ \\ 
	\textbullet $q = 1-p$\\ 
	\textbullet $m'_X(t) = npe^t(pe^t+q)^{n-1}$ \\ 
	\textbullet $m''_X(t) = npe^t(npe^t+q)(pe^t+q)^{n-2}$}
\item{\textbf{Geometric (nbr. rep. until $1^{st}$ succ.)}\\ 
	$X\sim Ge(p)$\\
	$P(X>a)= q^a$
}
\item{\textbf{Poisson (nbr. occ. in speficied boundaries):} 
	$X\sim Po(\lambda)$ \\ 
	\textbullet Homogen($\lambda = cst$), Indep., Rare($\lim_{n \rightarrow \infty} p =0$) \\ 
	\textbullet $Bi(n,\dfrac{\lambda}{n}) \Rightarrow Po(\lambda)$ (for $n \Rightarrow \infty$) \\
	if $n\gg$, $p\ll$ and $\lambda=np<7$ then\\ $Bi(n,p)\approx Po(np)$ \\ 
} 
\item{\textbf{Négative binomial}\\
(nbr. rep. until r succes)  
}
\item{\textbf{Hyper géométrique}\\
(n parmis N,r succes; X= nbre de sous ensemble)  
}
\end{itemize}


\textbf{Continuous distributions}\\
$P(X>a) = P(X \geq a)$ car $P(X=a) = 0$
\begin{itemize}
\item{\textbf{Uniform:}\\
	$f(X)=\frac{1}{b-a}$ if $X \in [a,b]$ (from $\int_a^b f(x)dx = 1$)}
\item{\textbf{Normal:} if $X\sim N(\mu, \sigma^2)$ \\ 
	\textbullet $P(Z\leq a)=1-P(Z\geq a)$\\
	\textbullet $P(Z\leq -a)= P(Z\geq a)$\\
	\textbullet $N(\mu_1, \sigma_1^2) + N(\mu_2, \sigma_2^2) \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) $
}
\item{\textbf{Standard Normal:} $Z\sim N(0,1)$\\ 
	\textbullet Normal with $\mu=0$ and $\sigma=1$ \\ 
	for $Z\sim N(0,1)$ and $\alpha \in (0,1)$: \\ 
	\textbullet $z_{\alpha} = q_{1-\alpha}\mid P(Z\geq z_{\alpha})=\alpha$ \\ 
	\textbullet $z_{\alpha}=-z_{1-\alpha}$ \\ 
	\textbullet if $X\sim N(\mu, \sigma^2)$ then $a+bX\sim N(a+b\mu, b^2\sigma^2)$ and : \\  
	if $X\sim N(\mu, \sigma^2)$ then $Z=\dfrac{X-\mu}{\sigma}\sim N(0,1)$ \\ 
	\textbullet if $X \sim N(\mu, \sigma^2)$: \\  
	$P(X\leq x) = P(\dfrac{X-\mu}{\sigma} \leq \dfrac{x-\mu}{\sigma}) = P(Z\leq \dfrac{x-\mu}{\sigma})$ \\ 
	\textbullet $\alpha$-percentage of $N(\mu,\sigma^2) = \sigma z_{\alpha} + \mu$ \\ 
	\textbullet $P(\mid X - \mu \mid \leq k\sigma) \geq 1 -\dfrac{1}{k^2}$ \\ 
	in partucal if $k=4.47$ then $P(X \in \left[ \mu - k \sigma , \mu + k \sigma \right] ) \geq 0.95 \approx 1$}
\item{\textbf{Gamma:} \\
	$X\sim \gammad(\alpha, \beta)$ \\
	\textbullet $\displaystyle{f(x) = \frac{x^{\alpha-1} \exp(\frac{-x}{\beta})}{\beta^\alpha \Gamma(\alpha)}}$ \\
	\textbullet $\Gamma (\alpha) = \displaystyle{\int_0^{\infty} t^{\alpha-1} e^{-t} dt}$ (fct gamma)\\ 
	\textbullet $\Gamma (\alpha+1)=\alpha \Gamma (\alpha)$ \\
	\textbullet $\Gamma (1)=1$ \\ 
	\textbullet $\Gamma (n+1)=n!$ \\
	\textbullet $cX\sim \gammad(\alpha,c \beta)$}
\item{\textbf{Exponential:} \\ 
	\textbullet Defining the lifetime of a event or the waiting time  for an occurence of a event\\
	\textbullet $X\sim Expo(\beta) \sim \gammad(1,\beta)$
}
\item{\textbf{Chi-square:} \\ 
	\textbullet $X\sim \chi^2_n \sim \gammad(n/2,2)$
}
\end{itemize}

\columnbreak
\textbf{\footnotesize Multivariable dist. for the vector (X,Y)}

\begin{itemize}
\item{\textbf{Joint distribution:} \\ 
	\textbullet joint pmf: $p(x,y)=P(X=x, Y=y)$ \\ 
	\textbullet joint pdf: $f(x,y)$ as \\
	$ f\geq0$ and $\displaystyle{\int_{\infty}^{\infty} \int_{\infty}^{\infty} f(x,y) dxdy =1}$ \\
	\textbullet joint cdf $F(x,y)=P(X\leq x, Y\leq y)$: \\ 
	\textbf{(dis.)} $F(x,y)= \displaystyle{\sum_{s\leq x, t\leq y} p(s,t)}$ \\ 
	\textbf{(cont.)} $F(x,y)=\displaystyle{\int_{\infty}^y \int_{\infty}^x f(s,t) dsdt}$ \\ 
	\textbullet $f(s,t)= \dfrac{\partial ^2 F}{\partial x \partial y}(s,t)$
}
\item{\textbf{Marginal}: \\
	\textbullet if $X,Y$ have joint pdf: \\ 
	(dis.) $p_X(x) = P(X=x)=\displaystyle{\sum_y P(X=x,Y=y)}$ \\ 
	(con.) $f_X(x) = \displaystyle{\int_{-\infty}^{\infty} f(x,y)dy}$ \\ 
	\textbullet if $X,Y$ have joint cdf: \\ $F_X(x) = P(X\leq x)= F(x, \infty)$
}
\item{\textbf{Conditionnal}:\\
	\textbf{discrete} \\ 
	\textbullet  $p(y\mid x)=P(Y=y \mid X=x) = \dfrac{p(x,y)}{p_X(x)}$ \\ 
	\textbullet $P(Y\in B \mid X=x)= \displaystyle{\sum_{y\in B} p(y\mid x)}$ \\ 
	\textbullet $F(y\mid x)=P(Y\leq y \mid X=x)= \displaystyle{\sum_{t\leq y} p(t\mid x)}$ \\ 
	\textbf{conti.} \\ \textbullet $P(Y \in B, X=x)= \displaystyle{ \int_B f(y\mid x)dy}$ \\ 
	\textbullet $F(y\mid x)=P(Y\leq y \mid X=x)= \displaystyle{\int_{\infty}^y f(t \mid x) dt}$ \\ 
	\textbullet $\dfrac{\partial F}{\partial y}(y\mid x)=f(y\mid x)$\\
	\\
 	\textbullet if $P(X\in A)>0$, $P(Y \in B \mid X\in A) = $ \\ 
 	\textbf{(disc.)} $\displaystyle{\sum_{y\in B} \dfrac{\sum_{x\in A} p(x,y)}{\sum_{x \in A} p_X(x)}}$\\
	\textbf{(cont.)} $\displaystyle{\int_B \dfrac{\int_A f(x,y)dx}{\int_A f_X(x)dx} dy}$\\
	\textbullet \begin{empheq}[left={\E(g(Y) \mid X = x)= \empheqlbrace}]{align} & \sum_y g(y)p(y\mid x)  \\ 
	& \int g(y)f(y\mid x) dy \end{empheq} \\ 
	\textbullet $\mu_Y = \E(Y\mid X=x)$ \\ 
	\textbullet $\sigma^2_Y(x)\equiv \var(Y\mid X=x) =$\\$ \E(Y^2\mid X=x) - \mu^2_Y(x)$
}
\item{\textbf{Momentum:} \\ 
	\textbullet $m_{X,Y}(s,t)=m(s,t)=\E(e^{sX+tY})$ \\ 
	\textbullet $\dfrac{\partial ^{k+n}m}{\partial s^k \partial t^n}(0,0) = \E(X^kY^n)$ \\ 
	\textbullet $m(s,0)=$ marg. mgf of X \\ 
	\textbullet $m(0,s)=$ marg. mgf of Y
}
\item{\textbf{Covariance} \\ 
	\textbullet $\sigma_{XY}\equiv \cov(X,Y) = \E[(X-\E(X))(Y-\E(Y))] = \E(XY)-\E(X)\E(Y)$ \\ 
	\textbullet $\cov(X,Y)=\cov(Y,X)$ \\ 
	\textbullet $\cov(X,X)=\var(X)$ \\ 
	\textbullet $\cov(a,X)=0$\\
	\textbullet $X \indep Y \Rightarrow \cov(X,Y)=0$\\
	\textbullet $\cov(aX+bY,Z)=a\cov(X,Z) + b\cov(X,Y)$\\
	\textbullet $\cov(a + bX,c + dY)=bd \cov(Y,Z)$\\
	\textbullet $<0$: X et Y varient dans des directions $><$\\
	\textbullet $>0$: X et Y varient dans même direction\\
	\textbullet $\mid \sigma_{XY} \mid \leq \sigma_X \sigma_Y$ (Cauchy-swartz)\\
	\textbullet $\mid \sigma_{XY} \mid = \sigma_X \sigma_Y$ si $Y = a+bX$\\
}
\item{\textbf{Correlation}: \\ 
	\textbullet $\rho_{XY}\equiv \rho(X,Y)=\dfrac{\sigma_{XY}}{\sigma_X \sigma_Y}$ \\ 
	\textbullet $\mid \rho_{XY} \mid \leq 1$ (Cauchy-swartz) \\ 
	\textbullet $\rho_{XY}=\pm 1 \Leftrightarrow Y = a+bX$}
\item{\textbf{Mutlinomial dist}: \\ 
	\textbullet $P(Y_1=y_1, ..., Y_n=y_n) = \dfrac{n!}{n_1!...n_n!}p_1^{n_1}...p_n^{n_n}$}
\end{itemize}


\textbf{Independant r.v. $X \indep Y$}
\begin{itemize}
\item[\textbullet]{$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$}
\item $F(x,y) = F_X(x)F_Y(y)$  
\item $f(x,y)=f_X(x)f_Y(y)$ 
\item $f(y\mid x)=f_Y(y)$, $f(x\mid y)=f_X(x)$
\item $h(X) \indep g(Y)$
\item $\E(XY) = \E(X)\E(Y)$\\ 
	$\E(g(X)h(Y)) = \E(g(X))\E(h(Y))$
\item $\var(X+Y) = \var(X) + \var(Y)$
\item $m_{X+Y}(t) = m_X(t) m_Y(t)$\\
$m_{\sum X_i}(t) = \prod m_{X_i}(t)$ si $X_1, ...$ ind.\\
\item $\sigma_{XY} = 0 = \rho_{XY}$
\end{itemize}


\textbf{Function of rv and transformation}

\begin{itemize}
\item{\textbf{Monotomic function}: \\ 
	\textbullet $f_Y(y)=\dfrac{f_X(x)}{|g'(x)|} |_{x=g^{-1}(y)}$ with $y \in Im(g)$
}
\item{\textbf{Lognormale dist}: if $X\sim N(\mu, \sigma^2)$,  $Y=e^X \sim LN(\mu, \sigma^2)$ and fdp. of $Y$= \\
	$f(y)=\dfrac{1}{y\sqrt{2\pi \sigma^2}}e^{-\dfrac{1}{2}\left( \dfrac{\ln y - \mu }{\sigma} \right)^2}$}
\item{\textbf{Chi squarred dist} if $X\sim N(0,1)$ and $Y=X^2$ then $Y\sim \chi^2_1$ (v=2)}
\item{\textbf{Bivarate transf}: \\ 
	\textbullet $f_Y(y) = \dfrac{f_X(x)}{\mid J_g(x)\mid} |_{x=g^-1(y)'}$ \\ 
	\textbullet $|J_g(x)|= \left| \dfrac{\partial g_1}{\partial x_1}\dfrac{\partial g_2}{\partial x_2}-\dfrac{\partial g_1}{\partial x_2}\dfrac{\partial g_2}{\partial x_1}\right| (x_1,x_2)$}
\end{itemize}
\begin{align*}
F_{X+Y}(z) &= \int_{-\infty}^\infty \int_{-\infty}^{z-y} f(x,y) \dif x \dif y\\
           &= \int_{-\infty}^\infty \int_{-\infty}^{z-x} f(x,y) \dif y \dif x \\
f_{X_1+X_2}(y_1) &= \int_{-\infty}^\infty f_X(y_1-y_2,y_2) \dif y_2\\
  &= \int_{-\infty}^\infty f_X(y_2,y_1-y_2) \dif y_2 \\
F_{XY}(z) &= \int_{-\infty}^0 \int_{z/y}^{\infty} f(x,y) \dif x \dif y\\
  &\quad+ \int_{0}^\infty \int_{-\infty}^{z/y} f(x,y) \dif x \dif y \\
f_{X_1X_2}(y_1) &= \int_{-\infty}^\infty \frac{1}{|y_2|}f_X(y_1/y_2,y_2) \dif y_2 \\
F_{X/Y}(z) &= \int_{-\infty}^0 \int_{yz}^{\infty} f(x,y) \dif x \dif y\\
           &\quad+ \int_{0}^\infty \int_{-\infty}^{yz} f(x,y) \dif x \dif y \\
f_{X_1/X_2}(y_1) &= \int_{-\infty}^\infty |y_2|f_X(y_1y_2,y_2) \dif y_2
\end{align*}
if $X \sim \mathcal{N}(0,1)$,$Y \sim \mathcal{N}(0,1)$ (normales standards), $X,Y$ ind. and $Z = Y/X$,
\begin{align*}
  f_Z(z) & = \frac{1}{\pi}\int_0^\infty ye^{-y^2(z^2+1)/2} \dif y\\
         & = \frac{1}{2\pi}\int_0^\infty e^{-t(z^2+1)/2} \dif t\\
         & = \frac{1}{\pi(z^2+1)}, -\infty < z < \infty.
\end{align*}
This is called standard Cauchy density.
$\E(Z)$ and $\var(Z)$ are undefined.

\textbf{Sampling distributions}


\begin{itemize}
\item{\textbf{Mean \& Variance:} For $X_i$,i=1...n \\ \textbullet $\mu=\E(X_i)$, $\sigma^2=\var(X_i)$ \\ \textbullet $\eta_4 = \E((X_i-\mu)^4)$ \\ \textbullet $\E(\overline{X})=\mu$ and $\var(\overline{X})=\dfrac{\sigma^2}{n}$ \\ \textbullet $\E(S^2)=\sigma^2$ and $\var(S^2)=\dfrac{1}{n}\left( \eta_4 - \dfrac{n-3}{n-1}\sigma^4 \right)$ \\ \textbullet if $X\sim Be(p)|\hat{p_n} = n^{-1}\displaystyle{\sum_{i=1}^n X_i}$, then: \\ \textbullet $\E(\hat{p_n})=p$, $\var(\hat{p_n})=\dfrac{p(1-p)}{n}$}
\item{\textbf{Normal population} 
	if $X\sim N(\mu, \sigma^2)$ then: \\ 
	\textbullet $\dfrac{\overline{X}-\mu }{\sigma /\sqrt{n}} \sim N(0,1)$ \\ 
	\textbullet $\overline{X} \indep S^2$ \\ 
	\textbullet $(n-1)S^2/\sigma^2 \sim \chi^2_{n-1}$ \\ 
	\textbullet $\dfrac{\overline{X}-\mu }{S /\sqrt{n}} \sim t_{n-1}$
}
\item{\textbf{Central Limit th. CLT} if $n\Rightarrow \infty$\\ 
	\textbullet $Z_n = \sqrt{n}\dfrac{\overline{X_n}-\mu}{\sigma} \Rightarrow Z \sim N(0,1)$ \\ 
	for $n\gg$ \\ 
	\textbullet $\displaystyle{\sum^n_{i=1} X_i \approx N(n\mu , n\sigma^2)}$
}
\item{\textbf{Normal approx } \\
	\textbullet $\bin(n,p)\approx N(np,np(1-p))$ if $n>9\dfrac{\max(p,q)}{\min(p,q)}$\\ 
	\textbullet $\po(\lambda) \approx N(\lambda , \lambda)$, ($\lambda\gg$) \\ 
	\textbullet $Gamma(\alpha, \beta) \approx N(\alpha \beta , \alpha \beta ^2)$,($\alpha \gg$) \\ 
	\textbullet $\chi^2_n \approx N(n,2n)$,($n\gg$)
}
\item{ si $Y \sim Expo(\theta)$, alors $min(Y) \sim Expo
(\theta / n)$
}
\end{itemize}


\textbf{Point estimations}
\begin{itemize}
\item{\textbf{Joint pdf.} for \textbf{X}($X_1$,...)\\ \textbullet $f_n(x;\theta) = \displaystyle{\prod_{i=1}^n f(x_i; \theta)}$}
\item{\textbf{Bias and MSE (mean square error)} \\ \textbullet Unbiased if $\E(\hat{\theta}) = \theta$ \\ \textbullet $\bias(\hat{\theta}) = \E(\hat{\theta})-\theta$ \\ \textbullet $\mse(\hat{\theta})=E\left[ \left( \hat{\theta} - \theta \right) ^2 \right]$ \\ $=\bias^2(\hat{\theta}) + \var(\hat{\theta})$}
\item{\textbf{Consestency} $\hat{\theta}$ cnst. if : \\ \textbullet $P(\mid \hat{\theta} - \theta \mid > \epsilon) \Rightarrow 0$, $n\Rightarrow \infty$ \\ \textbullet $\mse(\hat{\theta}) \Rightarrow 0$ \\ \textbullet if $\hat{\theta}$ cnst. for $\theta$,$g(\hat{\theta})$ cnst. for $g(\theta)$}
\item{\textbf{Method of Moments} \\ \textbullet $\mu_k = \E(X^k)$ \\ \textbullet $\hat{\mu_k}=n^{-1} \sum_{i=1}^n X_i^k$ \\ Solve $\Rightarrow \hat{\mu_k} = \mu_k(\theta)$ in $\theta$}
\item{\textbf{Maximum Likelihood} \\ 
	\textbullet $L_n(\theta )=f_n(x_1,...,x_n;\theta ) =_{iid} \displaystyle{\prod_{i=1}^n f(x_i;\theta )}$ \\ 
	\textbullet $LL_n(\theta ) = \displaystyle{\sum_{i=1}^n \ln(f(x_i;\theta ))}$ \\ 
	\textbullet Solve $S_n(\theta) = \left( \dfrac{\partial LL}{\partial \theta_1} (\theta), \dfrac{\partial LL}{\partial \theta_2} (\theta) \right)^T =0$ \\ 
	Check $2^{nd}$ to be sure of the maximum \\ 
	\textbullet $\hat{\theta}$ equivariant and consistant \\
	\textbullet MLE: maximum Likelihood estimator
}
\item{\textbf{pivot} f(U) ne dépend pas de $\Theta$

}
\end{itemize}

\textbf{Confidence interval}
\begin{itemize}
\item{\textbf{Goal}: \\ \textbullet $P(\hat{\theta_L} \leq \theta \leq \hat{\theta_U})=1-\alpha$}
\item{\textbf{Example}: \\ \textbf{Mean} \\ \textbullet $CI=\left[ \overline{X}-z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}, \overline{X}+z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}} \right]$ \\ and $l=2z_{\alpha/2}\dfrac{\sigma}{\sqrt{n}}$ (not random!)}
\item{\textbf{Deriving a CI} for $Z_n(\theta )\equiv Z(\textbf{X},\theta )$\\ \textbullet $P(a\leq Z_n(\theta ) \leq b)=1-\alpha$ \\ $\Rightarrow P(L\leq \theta \leq U)=1-\alpha$}
\end{itemize}

\textbf{Test d'hypothèse}
\begin{itemize}
\item erreur type 1 ETI: $RH_0|H_0$, P(ETI) = $\alpha$
\item erreur type 2 ETII: $AH_0|H_1$, P(ETII) = $\beta$
\item région de rejet R: [0,c]
\item{\textbf{single mean} \\
	\textbullet $\mu,\sigma$ unknown\\
	\textbullet $H0: \mu_1 = \mu_0$\\
	\textbullet under $H_0, \displaystyle{T = \frac{(\overline{X})- \mu_0}{S/ \sqrt{n}}} \sim t_{n-1,?}$
	\begin{tabular}{|c|c|}
	\hline
	$H_1$ & région de rejet\\
	\hline \hline
	$\mu_1 \neq \mu_0$ & $|T| > t_{n-1,\alpha /2}$\\
	\hline
	$\mu_1 > \mu_0$ & $T > t_{n-1,\alpha}$\\
	\hline
	$\mu_1 < \mu_0$ & $T < -t_{n-1,\alpha}$\\
	\hline
	\end{tabular}
}
\item{\textbf{T-test} \\
	\textbullet $\mu_1,\mu_2$, same unknown $\sigma^2$\\
	\textbullet $H0: \mu_1 -\mu_2 = \delta$\\
	\textbullet under $H_0, \displaystyle{T = \frac{(\overline{X}_1 -\overline{X}_2)- \delta}{S_p \sqrt{1/n_1 + 1/n_2}}} \sim t_{n1+n2-2,?}$
	\begin{tabular}{|c|c|}
	\hline
	$H_1$ & région critique\\
	\hline \hline
	$\mu_1 -\mu_2 \neq \delta$ & $|t| > t_{n1+n2-2,\alpha /2}$\\
	\hline
	$\mu_1 -\mu_2 > \delta$ & $t > t_{n1+n2-2,\alpha}$\\
	\hline
	$\mu_1 -\mu_2 < \delta$ & $t < -t_{n1+n2-2,\alpha}$\\
	\hline
	\end{tabular}
}
\item{\textbf{F-test} \\
	\textbullet $\sigma_1,\sigma_2$\\
	\textbullet $H0: \sigma_1 = \sigma_2$\\
	\textbullet under $H_0, \displaystyle{F = \frac{S_1^2}{S_2^2}} \sim F_{n1-1,n2-1,?}$
}
\item{\textbf{p-valeur}
$p_n(x)=P(T_n(X)<T_n(x)|H_0)$
}
\end{itemize}

\textbf{Additionnal informations}

\begin{itemize}
\item{\textbf{Geometric serie sum:} \\ 
	\textbullet $\displaystyle{\sum_{i=0}^n q^i} = \dfrac{1-q^n}{1-q}$ si $|q| < 1$
}
\item{\textbf{Tchebyshev theorem:} \\ \textbullet $P( | X- \mu |\geq \sigma k) \leq \dfrac{1}{k^2}$}
\item{\textbf{Markov theorem:} \\ \textbullet $P(|X|\geq a)\leq \dfrac{E|X|}{a}$}
\item \textbf{Sum} $X_i, i= 1,...,n$\\
	\begin{tabular}{|c|c|}
	\hline
	$X_i$ & somme\\
	\hline \hline
	$Bi(n_i,p$) & $Bi(n,p)$\\
	\hline
	$Po(\lambda_i)$ & $Po(\sum\lambda_i)$\\
	\hline
	$Gamma(\alpha_i,\beta)$ & $Gamma(\sum \alpha_i,\beta)$\\
	\hline
	$Expo(\beta)$ & $Gamma(n,\beta)$\\
	\hline
	$\chi^2_{ni}$ & $\chi^2_{\sum ni}$ \\
	\hline
	$N(\mu_i,\sigma_i^2)$ & $N(\sum \mu_i,\sum \sigma_i^2)$ \\
	\hline
	\end{tabular}
\end{itemize}

\end{multicols*}

\end{document}
